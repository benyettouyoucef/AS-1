{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2016-10-13T22:10:40.750Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T00:31:15",
     "start_time": "2016-10-13T22:31:15.032Z"
    }
   },
   "source": [
    "First we are about to write down a script that adapt the mnist dataset to our needs, meaning :\n",
    "\n",
    "* Transform mnist dataset into a 60000 x 784 instead of a 60000 x 28 x 28 tensor\n",
    "* Normalize data between 0 and 1\n",
    "* Return as shuffled version of the dataset with the corresponding labels, renaming classes as -1 or +1.\n",
    "\n",
    "We will then consider two classes of the mnist dataset to make our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:51:33",
     "start_time": "2016-10-17T10:51:33.854Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:51:34",
     "start_time": "2016-10-17T10:51:34.010Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = require 'mnist'\n",
    "require 'nn';\n",
    "train = mnist.traindataset()\n",
    "labels = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:51:34",
     "start_time": "2016-10-17T10:51:34.013Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reshaped_data = torch.reshape(train.data, 60000,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-14T14:57:20",
     "start_time": "2016-10-14T12:57:20.741Z"
    }
   },
   "source": [
    "## Shuffling the dataset using a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:02:53",
     "start_time": "2016-10-17T11:02:53.644Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function mnist_munging(data,labels)\n",
    "    -- We are going to time the script. It is a good habit to have.\n",
    "    timer = torch.Timer()    \n",
    "    times = {}\n",
    "    times[\"reshaping\"]  = timer:time().real\n",
    "    -- Reshaping and normalizing\n",
    "    local reshaped_data = reshaped_data:double()\n",
    "    reshaped_data = reshaped_data/torch.max(reshaped_data)\n",
    "    times[\"reshaping\"] = timer:time().real - times[\"reshaping\"]\n",
    "    print(\"times : \")\n",
    "    print(times)\n",
    "    return reshaped_data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:03:55",
     "start_time": "2016-10-17T11:03:55.576Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function classify_training_examples(reshaped_data,labels)\n",
    "    -- Now we will gather the training examples by labels.\n",
    "    timer = torch.Timer()    \n",
    "    times = {}\n",
    "    times[\"classifying\"]  = timer:time().real\n",
    "    -- basic type checking  -- TODO Doesn't work right now.\n",
    "    if type(labels) == nil then\n",
    "        return \"please provide some good labels\"\n",
    "    end\n",
    "    \n",
    "    -- We create the appropriate tensors in order to stock the training examples\n",
    "    local classified_examples = {}    \n",
    "    for i=0,9 do\n",
    "        classified_examples[i] = {}\n",
    "        classified_examples[i][\"data\"] = {} \n",
    "        classified_examples[i][\"count\"] = 0\n",
    "    end\n",
    "    \n",
    "    for i=1,(#reshaped_data)[1] do\n",
    "        classified_examples[labels[i]][\"count\"] = classified_examples[labels[i]][\"count\"] + 1\n",
    "        classified_examples[labels[i]]['data'][classified_examples[labels[i]][\"count\"]] = reshaped_data[i]\n",
    "    end\n",
    "    times[\"classifying\"] = timer:time().real - times[\"classifying\"]\n",
    "    print(\"times : \")\n",
    "    print(times)\n",
    "    return classified_examples\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:47:06",
     "start_time": "2016-10-17T11:47:06.295Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function convert_to_tensor(data_table)\n",
    "    -- This method convert the data type from a table to a Tensor\n",
    "    local result_tensor = torch.Tensor(#data_table,784)\n",
    "    for i=1,#data_table do\n",
    "        result_tensor[i] = data_table[i]\n",
    "    end\n",
    "    return result_tensor\n",
    "end\n",
    "\n",
    "function get_target_classes(classified_dataset,class_1,class_2)\n",
    "    times = {}\n",
    "    timer = torch.Timer()\n",
    "    times[\"global\"] = timer:time().real\n",
    "    \n",
    "    --This method returns the targeted classes if they are included into the classes existing in the dataset.\n",
    "    print(\"# of example of class \".. class_1 .. \" : \" .. classified_dataset[class_1].count)\n",
    "    print(\"# of example of class \" .. class_2 .. \" : \" .. classified_dataset[class_2].count)\n",
    "    \n",
    "    -- We then create a dataset containing all the data with the correct label\n",
    "    \n",
    "    local trainset = {}\n",
    "    \n",
    "    local look_up_trainset = {}\n",
    "    -- filling up with class_1 examples \n",
    "    for i=1,classified_dataset[class_1].count do \n",
    "        look_up_trainset[i] = {}\n",
    "        look_up_trainset[i][\"data\"] = classified_dataset[class_1].data[i]\n",
    "        look_up_trainset[i][\"labels\"] = class_1\n",
    "    end\n",
    "    -- filling up with class_1 examples \n",
    "    for i=1,classified_dataset[class_2].count do \n",
    "        look_up_trainset[classified_dataset[class_1].count+i] = {}\n",
    "        look_up_trainset[classified_dataset[class_1].count+i][\"data\"] = classified_dataset[class_2].data[i]\n",
    "        look_up_trainset[classified_dataset[class_1].count+i][\"labels\"] = class_2\n",
    "    end\n",
    "    \n",
    "    times[\"look_up_building\"] = timer:time().real - times[\"global\"]\n",
    "    times[\"global\"] = timer:time().real\n",
    "    \n",
    "    -- We then shuffle the lookup trainset and the labels using the same permutation\n",
    "    total = classified_dataset[class_1].count + classified_dataset[class_2].count\n",
    "    -- permutation template\n",
    "    perm = torch.randperm(total)    \n",
    "    \n",
    "    local shuffled_trainset = {}\n",
    "    shuffled_trainset[\"data\"] = {}\n",
    "    shuffled_trainset[\"labels\"] = {}\n",
    "    \n",
    "    for i=1,total do \n",
    "       table.insert(shuffled_trainset[\"data\"],torch.Tensor(look_up_trainset[perm[i]][\"data\"]))\n",
    "        if look_up_trainset[perm[i]][\"labels\"] == class_1 then\n",
    "            shuffled_trainset[\"labels\"][i] = 1\n",
    "        else \n",
    "            shuffled_trainset[\"labels\"][i] = -1\n",
    "        end\n",
    "    end\n",
    "    shuffled_trainset[\"labels\"] = torch.Tensor(shuffled_trainset[\"labels\"])\n",
    "    shuffled_trainset[\"data\"] = convert_to_tensor(shuffled_trainset[\"data\"])\n",
    "\n",
    "    times[\"shuffling\"] = timer:time().real - times[\"global\"]\n",
    "    -- Adding metatable with __index function allowing heritage from torch\n",
    "    \n",
    "    \n",
    "    setmetatable(shuffled_trainset,\n",
    "    {__index = function(t, i)\n",
    "                return {t.data[i], t.labels[i]}\n",
    "               end})\n",
    "    function shuffled_trainset:size()\n",
    "        return self.data:size()\n",
    "    end\n",
    "    \n",
    "    print(\"times : \")\n",
    "    print(times)\n",
    "    return shuffled_trainset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:47:09",
     "start_time": "2016-10-17T11:47:07.539Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "times : \t\n",
       "{\n",
       "  reshaping : 0.94750618934631\n",
       "}\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "times : \t\n",
       "{\n",
       "  classifying : 0.34722208976746\n",
       "}\n",
       "# of example of class 0 : 5923\t\n",
       "# of example of class 3 : 6131\t\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "times : \t\n",
       "{\n",
       "  look_up_building : 0.0023889541625977\n",
       "  global : 0.0023980140686035\n",
       "  shuffling : 0.1449031829834\n",
       "}\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_data = mnist_munging(mnist.traindataset().data,mnist.traindataset().label)\n",
    "classified_data = classify_training_examples(reshaped_data,labels)\n",
    "training_set = get_target_classes(classified_data,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:45:55",
     "start_time": "2016-10-17T11:45:55.364Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = convert_to_tensor(training_set['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T11:27:12",
     "start_time": "2016-10-17T09:27:12.436Z"
    }
   },
   "source": [
    "As you can see we used a lot of loops during our algorithm. In Python, coding this way would not have been the optimal way.. But in LUA it is!\n",
    "Loops are optimized so we will use and abuse them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training \n",
    "\n",
    "We are about to implement a accuracy function that takes two arguments : first a list of prediction and second a list of the correct predictions and computes the prediction *accuracy* following the given formula : $$Acc =  \\frac{t_p + t_n}{t_p + f_p + f_n + t_n}$$\n",
    "\n",
    "\n",
    "With : \n",
    "$t_p$ and $t_n$ : Correctly labeled items\n",
    "$f_p$ and $f_n$ : Not correctly labeled items\n",
    "\n",
    "This seems plausible, since there are two actual classes in our classification problem, and our classifier attempts to label them  correctly. This is precisely the effectiveness measure often used for evaluating machine learning classification problems.\n",
    "\n",
    "We will later see that Accuracy might not be the best error evaluation method we have at hand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:51:38",
     "start_time": "2016-10-17T10:51:36.629Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  reshaping : 1.5437829494476\n",
       "}\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "# of example of class 1 : 6742\t\n",
       "# of example of class 3 : 6131\t\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "times : \t\n",
       "{\n",
       "  look_up_building : 0.083427906036377\n",
       "  global : 0.083441972732544\n",
       "  shuffling : 0.014678001403809\n",
       "}\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_data = mnist_munging(mnist.traindataset().data,mnist.traindataset().label)\n",
    "classified_data = classify_training_examples(reshaped_data,labels)\n",
    "training_set = get_target_classes(classified_data,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:51:38",
     "start_time": "2016-10-17T10:51:36.634Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Trying accuracy with fake labels prediction\n",
    "fake_predictions = torch.sign(torch.randn(#training_set[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:51:38",
     "start_time": "2016-10-17T10:51:36.637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function accuracy(y,out)\n",
    "    -- This method computes the accuracy of a set of prediction y, compared to some expected labels\n",
    "    local pred_signs = torch.sign(y)\n",
    "    local correct_classification = torch.cmul(pred_signs,out)\n",
    "    --local eval_params = torch.Tensor(1):fill(1):double()\n",
    "    return torch.mean(correct_classification:eq(1):double())\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:51:38",
     "start_time": "2016-10-17T10:51:36.641Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50244698205546\t\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy(fake_predictions,training_set[\"labels\"])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:59:03",
     "start_time": "2016-10-17T07:59:03.836Z"
    }
   },
   "source": [
    "## Batch and Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we specified in [this paper](../tme1/presentation/Gradient_Descent_Optimization Techniques.pdf), there are several different version of Gradient Descent. We distinguish two main versions of Gradient Descent algorithms :\n",
    "\n",
    "1. The **Batch** gradient Descent : Which is a version of Gradient Descent that takes all the training examples and optimize the model parameters all at once.\n",
    "1. The **Stochastic** gradient Descent, that update the model parameters and updates them using the training examples one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:21:54",
     "start_time": "2016-10-17T10:21:54.188Z"
    }
   },
   "source": [
    "#### Model\n",
    "\n",
    "In this section we will use the following model for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T14:24:43",
     "start_time": "2016-10-17T12:24:43.589Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Linear(784,1)\n",
    "criterion = nn.MSECriterion()\n",
    "model:zeroGradParameters() -- Here, we are setting the cummulative of the gradients parameters to be equal to 0\n",
    "model:reset() -- I don't know what this command is for but it was in the TP TODO : Look for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T14:24:45",
     "start_time": "2016-10-17T12:24:45.241Z"
    },
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "learning_rate= torch.random(1,10)/10000 -- We put a random weight parameter in this.\n",
    "maxEpoch = 50\n",
    "\n",
    "-- Well, might as well define a method so that we don't have to type it all the time\n",
    "function train_batch(params)\n",
    "    if params.times == nil then params.times = {} end\n",
    "    timer = torch.Timer()\n",
    "    params.times[\"batch_training\"] = timer:time().real\n",
    "    -- for k,_ in pairs(params) do print(k) end\n",
    "    --assert(params.data ~= nil, 'please provide correct structure for the training set (i.e. training_set.data ~= nil)')\n",
    "    --assert(params.labels ~= nil, 'please provide correct structure for the training set (i.e. training_set.labels ~= nil)')\n",
    "    local batch_loss = {}    \n",
    "    model:zeroGradParameters()\n",
    "    for iteration=1,maxEpoch do\n",
    "        loss=0\n",
    "        output = model:forward(params.training_set.data)\n",
    "        loss= criterion:forward(output,params.training_set.labels)\n",
    "        model:backward(params.training_set.data,criterion:backward(model.output,params.training_set.labels))\n",
    "        updated_parameters = model:updateParameters(learning_rate)\n",
    "        batch_loss[iteration] = loss\n",
    "    end\n",
    "    times[\"batch_training\"] = timer:time().real - params.times[\"batch_training\"]\n",
    "    print(\"times\")\n",
    "    print(times)\n",
    "    return batch_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T14:24:45",
     "start_time": "2016-10-17T12:24:45.912Z"
    },
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "batch_params = {}\n",
    "batch_params[\"learning_rate\"] = learning_rate\n",
    "batch_params[\"maxEpoch\"] = 100\n",
    "batch_params[\"training_set\"] = training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T14:25:30",
     "start_time": "2016-10-17T12:25:27.087Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss \n",
       "\t\n"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "times\t\n",
       "{\n",
       "  look_up_building : 0.0023889541625977\n",
       "  global : 0.0023980140686035\n",
       "  batch_training : 2.9678781032562\n",
       "  shuffling : 0.1449031829834\n",
       "}\n"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"loss \\n\")\n",
    "train_batch(batch_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T13:51:24",
     "start_time": "2016-10-17T11:51:21.903Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "times\t\n",
       "{\n",
       "  look_up_building : 0.0023889541625977\n",
       "  global : 0.0023980140686035\n",
       "  batch_training : 2.4079020023346\n",
       "  shuffling : 0.1449031829834\n",
       "}\n",
       "{\n",
       "  1 : 0.98453662733588\n",
       "  2 : 0.98144559856183\n",
       "  3 : 0.97529596882648\n",
       "  4 : 0.96615191033704\n",
       "  5 : 0.95410799769915\n",
       "  6 : 0.93928725595979\n",
       "  7 : 0.92183866899693\n",
       "  8 : 0.90193423724641\n",
       "  9 : 0.87976568863593\n",
       "  10 : 0.85554095704896\n",
       "  11 : 0.82948054823825\n",
       "  12 : 0.80181391363586\n",
       "  13 : 0.77277594797322\n",
       "  14 : 0.74260371726463\n",
       "  15 : 0.71153350996543\n",
       "  16 : 0.67979828663048\n",
       "  17 : 0.64762558297154\n",
       "  18 : 0.61523589877338\n",
       "  19 : 0.58284158169471\n",
       "  20 : 0.55064619160869\n",
       "  21 : 0.51884430887988\n",
       "  22 : 0.48762172982815\n",
       "  23 : 0.45715597549626\n",
       "  24 : 0.4276170264778\n",
       "  25 : 0.39916818756863\n",
       "  26 : 0.37196698176858\n",
       "  27 : 0.34616597386028\n",
       "  28 : 0.3219134293803\n"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "  29 : 0.29935372500931\n",
       "  30 : 0.27862744076404\n",
       "  31 : 0.25987108221214\n",
       "  32 : 0.24321640142866\n",
       "  33 : 0.2287893076265\n",
       "  34 : 0.216708381301\n",
       "  35 : 0.20708302827624\n",
       "  36 : 0.20001133118501\n",
       "  37 : 0.19557767467785\n",
       "  38 : 0.19385023616196\n",
       "  39 : 0.19487844538774\n",
       "  40 : 0.19869052317516\n",
       "  41 : 0.20529121165133\n",
       "  42 : 0.21465980542066\n",
       "  43 : 0.22674858519997\n",
       "  44 : 0.24148174293314\n",
       "  45 : 0.25875487077124\n",
       "  46 : 0.27843506626674\n",
       "  47 : 0.30036168353871\n",
       "  48 : 0.32434773599189\n",
       "  49 : 0.35018193145759\n",
       "  50 : 0.37763129643967\n",
       "}\n"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T14:23:39",
     "start_time": "2016-10-17T12:23:39.863Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "require 'gnuplot'\n",
    "--require 'tools'\n",
    "Plot = require 'itorch.Plot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T14:23:42",
     "start_time": "2016-10-17T12:23:42.082Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "[string \"-- scatter plots ...\"]:2: attempt to index global 'cat_1' (a nil value)\nstack traceback:\n\t[string \"-- scatter plots ...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:210: in function </Users/david/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t/Users/david/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0100c6ad10",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- scatter plots ...\"]:2: attempt to index global 'cat_1' (a nil value)\nstack traceback:\n\t[string \"-- scatter plots ...\"]:2: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:210: in function </Users/david/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t/Users/david/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/david/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/david/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x0100c6ad10"
     ]
    }
   ],
   "source": [
    "-- scatter plots \n",
    "plot = Plot():circle(cat_1.x,cat_1.y, 'red', 'Category 1'):circle(cat_2.x, cat_2.y, 'blue', 'Category 2'):draw() \n",
    "plot:title('Data Points to separate'):redraw() \n",
    "plot:xaxis('random variable 1'):yaxis('random variable 2'):redraw() \n",
    "plot:legend(true) \n",
    "plot:save('out.html')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:40:13",
     "start_time": "2016-10-17T10:40:13.592Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:40:30",
     "start_time": "2016-10-17T10:40:30.270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:11:53",
     "start_time": "2016-10-17T10:11:53.424Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"-- 3 : Learning Loop...\"]:7: '}' expected near ':'",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"-- 3 : Learning Loop...\"]:7: '}' expected near ':'"
     ]
    }
   ],
   "source": [
    "-- 3 : Learning Loop\n",
    "learning_rate= torch.random(1,10)/10000 -- We put a random weight parameter in this.\n",
    "maxEpoch = 50\n",
    "all_losses={}\n",
    "-- Well, might as well define a method so that we don't have to type it all the time\n",
    "\n",
    "stoch_params = {\"learning_rate\":learning_rate,\"maxEpoch\":maxEpoch}\n",
    "\n",
    "function train_stochastic(training_set,learning_rate,maxEpoch)\n",
    "    if times ==nil then\n",
    "        times = {}\n",
    "        timer = torch.Timer()\n",
    "        times[\"stoch_training\"] = timer:time()\n",
    "    end\n",
    "    assert(training_set.data ~= nil, 'please provide correct structure for the training set (i.e. training_set.data ~= nil)')\n",
    "    assert(training_set.data ~= nil, 'please provide correct structure for the training set (i.e. training_set.label ~= nil)')\n",
    "    local stochastic_losses = {}    \n",
    "    model:zeroGradParameters()\n",
    "    for iteration=1,maxEpoch do\n",
    "        loss=0\n",
    "        for i=1,training_set.data:size()[1] do \n",
    "            output = model:forward(training_set.data[i])\n",
    "            loss= loss + criterion:forward(output,training_set.label[i])\n",
    "            model:backward(training_set.data[i],criterion:backward(model.output,training_set.label[i]))\n",
    "            updated_parameters = model:updateParameters(learning_rate)\n",
    "        end\n",
    "        stochastic_losses[iteration] = loss\n",
    "    end\n",
    "    times[\"stoch_training\"] = timer:time() - times[\"stoch_training\"]\n",
    "    print(\"times\")\n",
    "    print(times)\n",
    "    return stochastic_losses\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T10:21:27",
     "start_time": "2016-10-17T08:21:27.347Z"
    }
   },
   "source": [
    "# Train - Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "621px",
   "left": "0px",
   "right": "954.67px",
   "top": "131px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
